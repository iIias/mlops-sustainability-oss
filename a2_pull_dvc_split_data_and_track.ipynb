{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Pull Newest Full Data, make Train Test split and Track those."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Clean data\n","    - Drop columns not required for training\n","    - Drop rows with null valus where it makes sense \n","    (river discharge may be NaN where there is no river. It makes sense to keep these rows for the model to learn where rivers are)\n","- Think about whether or not to have separate notebooks for new data retrievals and prep\n","- Version Control the data\n","- Train test splitting\n","- Version control again??"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Install required packages.\n","# TODO: Create IBM Cloud Software Configuration for those\n","!pip install ibm-cos-sdk ibm_watson_studio_pipelines 'dvc[s3]' # dvc[all] alternatively, however, COS is covered by S3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from ibm_watson_studio_pipelines import WSPipelines\n","import ibm_boto3\n","\n","from botocore.client import Config\n","from sklearn.model_selection import train_test_split\n","from dataclasses import dataclass\n","import numpy as np\n","import pandas as pd\n","\n","import pickle\n","import dvc.api\n","import io\n","\n","import logging\n","import os, types\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Setup IBM Cloud and COS Credentials\n","\n","**Note**: If you are running this notebook outside of a Watson Studio Pipeline execution. Make sure to set the environment variables that the Pipeline environment would have passed to the notebook.\n","Refer to ```credentials.py```."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Uncomment this cell and put your credentials in credentials.py to run locally.\n","from credentials import set_env_variables_for_credentials\n","set_env_variables_for_credentials()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CLOUD_API_KEY = os.getenv(\"CLOUD_API_KEY\")\n","DATA_FILENAME = os.getenv(\"serialized_data_filename\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### DVC Pull and Deserialize Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: Make pipeline param\n","repo = \\\n","    os.getenv(\"GIT_REPOSITORY\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Retrieve dataset from tracking information in git. The repository itself contains the remote storage info and credentials.\n","data = pickle.load(io.BytesIO(dvc.api.read(f\"data/{DATA_FILENAME}\",repo=repo, mode=\"rb\")))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Replace NaN values with 0. \n","# Instead of dropping rows with NaN value alltogether, we want to keep them.\n","# Data here may indicate where there are no rivers (river discharge always = 0), (where it rarely rains = mostly 0 etc.)\n","\n","data.fillna(0, inplace=True)\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Assuming your large table is stored in a pandas DataFrame called 'df'\n","X = data.drop('dis24', axis=1)  # Extract input features by dropping the target column\n","y = data['dis24']  # Extract the target column\n","\n","\n","# Perform train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def serialize(obj, target_path):\n","    try:\n","        with open(target_path, 'wb') as _file:\n","            pickle.dump(obj, _file)\n","    except Exception as e:\n","        print(e)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_target = \"dvc-testing/data/train_package.pkl\"\n","\n","train_package = {}\n","train_package[\"X_train\"] = X_train\n","train_package[\"y_train\"] = y_train\n","\n","serialize(train_package, train_target)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_target = \"dvc-testing/data/test_package.pkl\"\n","\n","test_package = {}\n","test_package[\"X_test\"] = X_test\n","test_package[\"y_test\"] = y_test\n","\n","serialize(test_package, test_target)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["###  Setup DVC Situation\n","\n","Since we assume CPDaaS as environment, we will need to clone the dvc setup repository again.\n","Run the line shown below.\n","\n","```\n","!git clone https://[GIT_TOKEN]@github.com/[GIT_REPOSITORY].git\n","````\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# @hidden_cell\n","!git clone $GIT_REPOSITORY"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!cd dvc-testing && mkdir data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!cd dvc-testing && dvc get $GIT_REPOSITORY data/era5-glofas-merged.pkl -o data/era5-glofas-merged.pkl"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!cd dvc-testing && dvc add data/train_package.pkl data/test_package.pkl"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!cd dvc-testing && git add data/.gitignore data/train_package.pkl.dvc data/test_package.pkl.dvc"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!cd dvc-testing && git config --global user.email \"ilias.ennmouri@ibm.com\"\n","!cd dvc-testing && git config --global user.name \"Ilias Ennmouri\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!cd dvc-testing && git commit -m \"New train test subsets\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!cd dvc-testing && dvc push && git push"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: Pass to pipeline params\n","train_package_path = \"data/train_package.pkl\"\n","test_package_path = \"data/test_package.pkl\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","import numpy as np\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract the relevant columns for input features and target variable\n","input_columns = ['time', 'latitude', 'longitude', 'tp', 'swvl1', 'stl1', 'surface', 'valid_time']\n","target_column = 'dis24'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert non-numeric columns to numeric values\n","data['time'] = pd.to_datetime(data['time'])  # Convert dates to datetime objects\n","\n","\n","\n","#data['latitude'] = data['latitude'].astype('category').cat.codes  # Encode coordinates as categorical codes\n","#data['longitude'] = data['longitude'].astype('category').cat.codes  # Encode coordinates as categorical codes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# E.g. col 'step' has only a single unique value. Its existence has no effect on training is solely a waste of resources.\n","# Therefore we will drop all cols with that characteristic\n","for key in data.keys():\n","    if len(data[key].unique()) < 2:\n","        print(f\"col '{key}' dropped because it bears no more than one unique value.\")\n","        data = data.drop(key, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python3 -m pip install seaborn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","def generate_heatmap(X, y, descr='description'):\n","    df = pd.DataFrame(data=X.values, columns=X.columns.values, index=X.time.values)\n","    df['dis24'] = y\n","    plt.figure(figsize=(25,25))\n","    cor = df.corr()\n","    sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n","    plt.show()\n","    cor_predictand = abs(cor['dis24'])\n","    feature_importance = cor_predictand[cor_predictand > 0.2]\n","    print(descr)\n","    print(feature_importance)\n","    return feature_importance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["generate_heatmap(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":1}
