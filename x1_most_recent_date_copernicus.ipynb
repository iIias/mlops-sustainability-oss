{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Get Most Recent Date covered in current Dataset state"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This notebook\n","\n","- checks whether there already is data stored in our COS Bucket and tracked by DVC and\n","- if so, finds the most recent date covered by the data\n","\n","The most_recent_day covered will be passed on. Based on that, you will have to decide whether or not to pull newer data to supplement the current data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Install required packages.\n","# TODO: Create IBM Cloud Software Configuration for those\n","!pip install ibm_watson_studio_pipelines cdsapi 'dvc[s3]' # dvc[all] alternatively, however, COS is covered by S3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from ibm_watson_studio_pipelines import WSPipelines\n","\n","import pandas as pd\n","import xarray as xr\n","\n","import ibm_boto3\n","from botocore.client import Config\n","\n","import dvc.api\n","import cdsapi\n","import pickle\n","import io\n","\n","import json\n","import logging\n","import os, types\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Setup IBM Cloud and COS Credentials\n","\n","**Note**: If you are running this notebook outside of a Watson Studio Pipeline execution. Make sure to set the environment variables that the Pipeline environment would have passed to the notebook.\n","Refer to ```credentials.py```."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Uncomment this cell and put your credentials in credentials.py to run locally.\n","from credentials2 import set_env_variables_for_credentials\n","set_env_variables_for_credentials()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Retrieve cos credentials from global pipeline parameters\n","\n","# Get json from environment and convert to string\n","project_cos_credentials = json.loads(os.getenv('PROJECT_COS_CREDENTIALS'))\n","mlops_cos_credentials = json.loads(os.getenv('MLOPS_COS_CREDENTIALS'))\n","\n","## PROJECT COS \n","AUTH_ENDPOINT = project_cos_credentials['AUTH_ENDPOINT']\n","ENDPOINT_URL = project_cos_credentials['ENDPOINT_URL']\n","API_KEY_COS = project_cos_credentials['API_KEY']\n","BUCKET_PROJECT_COS = project_cos_credentials['BUCKET']\n","\n","## MLOPS COS\n","ENDPOINT_URL_MLOPS = mlops_cos_credentials['ENDPOINT_URL']\n","API_KEY_MLOPS = mlops_cos_credentials['API_KEY']\n","CRN_MLOPS = mlops_cos_credentials['CRN']\n","BUCKET_MLOPS  = mlops_cos_credentials['BUCKET']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CLOUD_API_KEY = os.getenv(\"CLOUD_API_KEY\")\n","GIT_REPOSITORY = os.getenv(\"GIT_REPOSITORY\")\n","REPO_NAME = os.getenv(\"REPO_NAME\")\n","\n","DATA_FILENAME = os.getenv(\"serialized_data_filename\")\n","MOST_RECENT_DATE = os.getenv(\"most_recent_date\") # Most recent date found in tracked dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TESTING\n","MOST_RECENT_DATE = '2023-04-30'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_df_to_cos(df,filename,key):\n","    \"\"\"\n","    \n","    Save Data in IBM Cloud Object Storage\n","\n","    \n","    \"\"\"\n","\n","    try:\n","        #df.to_csv(filename,index=False)\n","        with open(filename, 'wb') as file:\n","            pickle.dump(df, file)\n","        mlops_res = ibm_boto3.resource(\n","            service_name='s3',\n","            ibm_api_key_id=API_KEY_MLOPS,\n","            ibm_service_instance_id=CRN_MLOPS,\n","            ibm_auth_endpoint=AUTH_ENDPOINT,\n","            config=Config(signature_version='oauth'),\n","            endpoint_url=ENDPOINT_URL_MLOPS)\n","\n","        mlops_res.Bucket(BUCKET_MLOPS).upload_file(filename,key)\n","        print(f\"Dataframe {filename} uploaded successfully\")\n","    except Exception as e:\n","        print(e)\n","        print(\"Dataframe upload for {filename} failed\")\n","\n","\n","def check_if_file_exists(filename):\n","    mlops_client = ibm_boto3.client(\n","        service_name='s3',\n","        ibm_api_key_id=API_KEY_MLOPS,\n","        ibm_service_instance_id=CRN_MLOPS,\n","        ibm_auth_endpoint=AUTH_ENDPOINT,\n","        config=Config(signature_version='oauth'),\n","        endpoint_url=ENDPOINT_URL_MLOPS)\n","    \n","    for key in mlops_client.list_objects(Bucket=BUCKET_MLOPS)['Contents']:\n","        files = key['Key']\n","        if files == filename:\n","            return True\n","    return False"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Retrieve today's date and determine difference in days compared to most recent day covered by tracked dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datetime import datetime\n","\n","date_format = '%Y-%m-%d'\n","\n","today = datetime.now().date()\n","today_str = str(today)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["today"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["most_recent_date = datetime.strptime(MOST_RECENT_DATE, date_format).date()\n","most_recent_date"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["day_diff = (today - most_recent_date).days\n","day_diff"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use your Copernicus API_KEY\n","# @hidden_cell\n","CDS_USER_ID = os.getenv(\"CDS_USER_ID\")\n","CDS_API_KEY = os.getenv(\"CDS_API_KEY\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setup copernicus credentials file for cdsapi\n","with open(os.path.join(os.path.expanduser('~'), '.cdsapirc'), 'w') as f:\n","    f.write('url: https://cds.climate.copernicus.eu/api/v2\\n')\n","    f.write(f'key: {CDS_USER_ID}:{CDS_API_KEY}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Ensure COPERNICUS config is setup at the right place\n","!cat ~/.cdsapirc"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["copernicus = cdsapi.Client()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### (IF day_diff > 7) Get Copernicus Data between most_recent_date and today"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["europe = [72,25,34,40] # NWSE bounds for Europe\n","\n","days = [str(i+1) for i in range(31)]\n","# months = ['january', 'february', 'march', 'april']\n","# years = ['2023']\n","\n","all_months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n","months = all_months[most_recent_date.month:today.month] # Slice array to get missing months\n","\n","hours = [ '00:00',]\n","\n","years = [str(today.year)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_months.index(\"february\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["months"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["[str(all_months.index(month)+1) for month in months]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_newest_copernicus_data(client, bounds, years, months, days, download_path):\n","    ############ GloFAS #############\n","    glofas_format = \".netcdf4.zip\"\n","    glofas_new_filename = f'glofas-{download_path}{glofas_format}'\n","\n","    if os.path.exists(glofas_new_filename):\n","        # Reason to cancel download process if file exists is elaborated where method is invoked.\n","        print(f\"Target filename already exists in target path ({download_path}{glofas_format})... cancelling download\")\n","        exit\n","    else:\n","        client.retrieve(\n","        'cems-glofas-historical',\n","        {\n","            'system_version': 'version_3_1',\n","            'variable': 'river_discharge_in_the_last_24_hours',\n","            'format': 'netcdf4.zip',\n","            'hyear': years,\n","            'hmonth': months,\n","            'hday': days,\n","            'hydrological_model': 'lisflood',\n","            'product_type': 'intermediate',\n","            'area': bounds,\n","        },\n","        glofas_new_filename)\n","\n","        os.environ['glofas_new_filename'] = glofas_new_filename\n","        print(\"Stored GloFAS data\")\n","\n","\n","    ############ ERA5 #############\n","    era5_format = \".netcdf.zip\"\n","    era5_new_filename = f'era5-{download_path}{era5_format}'\n","\n","\n","    if os.path.exists(era5_new_filename):\n","        # Reason to cancel download process if file exists is elaborated where method is invoked.\n","        print(f\"Target filename already exists in target path ({download_path}{era5_format})... cancelling download\")\n","        exit\n","    else:\n","        client.retrieve(\n","                'reanalysis-era5-land',\n","                {\n","                    'variable': [\n","                        'soil_temperature_level_1', 'total_precipitation', 'volumetric_soil_water_layer_1',\n","                    ],\n","                    'year': years,\n","                    # CDS Datasets do not have uniformal requests. Here Months are expected to be e.g. \"01\" instead of 'january'.\n","                    # Work-around with list comprehension\n","                    # 'month': [str(i) for i in range(len(months))],\n","                    'month': [str(all_months.index(month)+1) for month in months], # converts 'january', 'february' to '1', '2'\n","                    'day': [f'0{i+1}' if i < 9 else str(i+1) for i in range(len(days))],\n","                    'time': hours,\n","                    'format': 'netcdf.zip',\n","                    'area': bounds,\n","                },\n","                era5_new_filename)\n","        \n","        os.environ['era5_new_filename'] = era5_new_filename\n","        print(\"Stored ERA5 data\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_newest_copernicus_data(\n","    copernicus,\n","    bounds=europe,\n","    years=years,\n","    months=months,\n","    days=days,\n","    download_path='-'.join(months) # Results in string of hyphen-separated months\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!ls -l | grep may"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Unpack ERA5/GloFAS data and prep it for merger with dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!mkdir era5_new && mkdir glofas_new"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!unzip $glofas_new_filename -d glofas_new"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!unzip $era5_new_filename -d era5_new"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["glofas_new = xr.open_dataset(\"glofas_new/data.nc\")\n","e5_new = xr.open_dataset(\"era5_new/data.nc\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use lat,long from glofas data (almost identical)\n","e5_new"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["glofas_new"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use lat,long from glofas data (almost identical)\n","# Also ensures that the same time span is used.\n","e5_interp = e5_new.interp_like(glofas_new)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Interpolate in case era5 data comes with additional expver mask over coordinates\n","if \"expver\" in e5_interp.coords.dims:\n","    e5_interp = e5_interp.sel(expver=1).combine_first(e5_interp.sel(expver=5))\n","    e5_interp.load()\n","    e5_interp"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Joining predictand onto feature y-interpolated table \n","# Set features to keep and choose target variable\n","X = e5_interp.to_dataframe()\n","y = glofas_new['dis24'].to_dataframe()\n","\n","# Reset the index to include the coordinates as columns\n","X.reset_index(inplace=True)\n","y.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Merge features and predictand together common coordinates (time, latitude, longitude)\n","data_new = pd.merge(X, y, on=['time', 'latitude', 'longitude'])\n","data_new"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["repo = \\\n","    GIT_REPOSITORY"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pickle.load(\n","    io.BytesIO(\n","        dvc.api.read(\n","            f\"data/era5-glofas-merged.pkl\",\n","            repo=repo, \n","            mode=\"rb\"\n","        )\n","    )   \n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Concatenate dataframe2 to dataframe1\n","concatenated = pd.concat([data, data_new])\n","\n","# Remove duplicate rows\n","deduplicated = concatenated.drop_duplicates()\n","\n","# Reset the index if needed\n","deduplicated.reset_index(drop=True, inplace=True)\n","\n","# Print the deduplicated dataframe\n","deduplicated"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["deduplicated.dropna(axis=0),"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filename = f\"updated-{DATA_FILENAME}\"\n","\n","save_df_to_cos(deduplicated, filename, filename)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Clean-up"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!rm -rf era5_new && rm -rf glofas_new"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!rm -rf $glofas_new_filename && rm -rf $era5_new_filename"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Set-up Credentials for Copernicus API"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_exists_and_newest_date = {}\n","data_exists_and_newest_date['copied_updated_data'] = check_if_file_exists(filename)\n","data_exists_and_newest_date['updated_data_filename'] = filename"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipelines_client = WSPipelines.from_apikey(apikey=CLOUD_API_KEY)\n","pipelines_client.store_results(data_exists_and_newest_date)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":1}
