{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Request Data from Copernicus (GloFAS; ERA5) and Save Merged Data to Cloud Object Storage (COS)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this initial notebook, we assume that when it is run, there is no existing data and we will download historic data for model training."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install cdsapi netCDF4 xarray ibm_watson_studio_pipelines"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from netCDF4 import Dataset\n","import xarray as xr\n","\n","import cdsapi\n","\n","from botocore.client import Config\n","from sklearn.model_selection import train_test_split\n","from dataclasses import dataclass\n","import numpy as np\n","import pandas as pd\n","\n","from ibm_watson_studio_pipelines import WSPipelines\n","import ibm_boto3\n","\n","import logging\n","import os, types\n","import warnings\n","import pickle\n","\n","warnings.filterwarnings(\"ignore\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Setup IBM Cloud and COS Credentials\n","\n","**Note**: If you are running this notebook outside of a Watson Studio Pipeline execution. Make sure to set the environment variables that the Pipeline environment would have passed to the notebook.\n","Refer to ```credentials.py```."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Uncomment this cell and put your credentials in credentials.py to run locally.\n","from credentials import set_env_variables_for_credentials\n","set_env_variables_for_credentials()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Retrieve cos credentials from global pipeline parameters\n","import json\n","# Get json from environment and convert to string\n","project_cos_credentials = json.loads(os.getenv('PROJECT_COS_CREDENTIALS'))\n","mlops_cos_credentials = json.loads(os.getenv('MLOPS_COS_CREDENTIALS'))\n","\n","## PROJECT COS \n","AUTH_ENDPOINT = project_cos_credentials['AUTH_ENDPOINT']\n","ENDPOINT_URL = project_cos_credentials['ENDPOINT_URL']\n","API_KEY_COS = project_cos_credentials['API_KEY']\n","BUCKET_PROJECT_COS = project_cos_credentials['BUCKET']\n","\n","## MLOPS COS\n","ENDPOINT_URL_MLOPS = mlops_cos_credentials['ENDPOINT_URL']\n","API_KEY_MLOPS = mlops_cos_credentials['API_KEY']\n","CRN_MLOPS = mlops_cos_credentials['CRN']\n","BUCKET_MLOPS  = mlops_cos_credentials['BUCKET']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CLOUD_API_KEY = os.getenv('CLOUD_API_KEY')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_df_to_cos(df,filename,key):\n","    \"\"\"\n","    \n","    Save Data in IBM Cloud Object Storage\n","\n","    \n","    \"\"\"\n","\n","    try:\n","        #df.to_csv(filename,index=False)\n","        with open(filename, 'wb') as file:\n","            pickle.dump(df, file)\n","        mlops_res = ibm_boto3.resource(\n","            service_name='s3',\n","            ibm_api_key_id=API_KEY_MLOPS,\n","            ibm_service_instance_id=CRN_MLOPS,\n","            ibm_auth_endpoint=AUTH_ENDPOINT,\n","            config=Config(signature_version='oauth'),\n","            endpoint_url=ENDPOINT_URL_MLOPS)\n","\n","        mlops_res.Bucket(BUCKET_MLOPS).upload_file(filename,key)\n","        print(f\"Dataframe {filename} uploaded successfully\")\n","    except Exception as e:\n","        print(e)\n","        print(\"Dataframe upload for {filename} failed\")\n","\n","def save_binary_to_cos(filename,key):\n","    \"\"\"\n","    \n","    Save Data in IBM Cloud Object Storage\n","\n","    \n","    \"\"\"\n","\n","    try:\n","        mlops_res = ibm_boto3.resource(\n","            service_name='s3',\n","            ibm_api_key_id=API_KEY_MLOPS,\n","            ibm_service_instance_id=CRN_MLOPS,\n","            ibm_auth_endpoint=AUTH_ENDPOINT,\n","            config=Config(signature_version='oauth'),\n","            endpoint_url=ENDPOINT_URL_MLOPS)\n","\n","        mlops_res.Bucket(BUCKET_MLOPS).upload_file(filename,key)\n","        print(f\"File {filename} uploaded successfully\")\n","    except Exception as e:\n","        print(e)\n","        print(\"File upload for {filename} failed\")\n","\n","def check_if_file_exists(filename):\n","    mlops_client = ibm_boto3.client(\n","        service_name='s3',\n","        ibm_api_key_id=API_KEY_MLOPS,\n","        ibm_service_instance_id=CRN_MLOPS,\n","        ibm_auth_endpoint=AUTH_ENDPOINT,\n","        config=Config(signature_version='oauth'),\n","        endpoint_url=ENDPOINT_URL_MLOPS)\n","    \n","    for key in mlops_client.list_objects(Bucket=BUCKET_MLOPS)['Contents']:\n","        files = key['Key']\n","        if files == filename:\n","            return True\n","    return False"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use your Copernicus API_KEY\n","# @hidden_cell\n","import os\n","CDS_USER_ID = os.getenv(\"CDS_USER_ID\")\n","CDS_API_KEY = os.getenv(\"CDS_API_KEY\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setup copernicus credentials file for cdsapi\n","import os\n","with open(os.path.join(os.path.expanduser('~'), '.cdsapirc'), 'w') as f:\n","    f.write('url: https://cds.climate.copernicus.eu/api/v2\\n')\n","    f.write(f'key: {CDS_USER_ID}:{CDS_API_KEY}')"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["# Ensure COPERNICUS config is setup at the right place\n","!cat ../.cdsapirc"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["copernicus = cdsapi.Client()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["europe = [72,25,34,40] # NWSE bounds for Europe\n","days = [str(i) for i in range(31)]\n","# months = ['january', 'february', 'march', 'april']\n","# years = ['2023']\n","\n","months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n","years = ['2023', '2022']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def download_glofas_historic(client, bounds, years, months, days, download_path):\n","    client.retrieve(\n","        'cems-glofas-historical',\n","        {\n","            'system_version': 'version_3_1',\n","            'variable': 'river_discharge_in_the_last_24_hours',\n","            'format': 'netcdf4.zip',\n","            'hyear': years,\n","            'hmonth': months,\n","            'hday': days,\n","            'hydrological_model': 'lisflood',\n","            'product_type': 'intermediate',\n","            'area': bounds,\n","        },\n","        f'{download_path}.netcdf4.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download ERA5 monthly averaged data from soil temp l1, volumetric soil water l1, total precipitation\n","def download_era5_historic(client, bounds, years, months, days, download_path):\n","    client.retrieve(\n","        'reanalysis-era5-land',\n","        {\n","            'variable': [\n","                'soil_temperature_level_1', 'total_precipitation', 'volumetric_soil_water_layer_1',\n","            ],\n","            'year': years,\n","            # CDS Datasets do not have uniformal requests. Here Months are expected to be e.g. \"01\" instead of 'january'.\n","            # Work-around with list comprehension\n","            # 'month': [str(i) for i in range(len(months))],\n","            'month': [f'0{i+1}' if i < 9 else str(i+1) for i in range(len(months))],\n","            'day': [f'0{i+1}' if i < 9 else str(i+1) for i in range(len(days))],\n","            'time': [\n","                '00:00'\n","            ],\n","            'format': 'netcdf.zip',\n","            'area': bounds,\n","        },\n","        f'{download_path}.netcdf.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["download_glofas_historic(copernicus,bounds=europe,years=years,months=months,days=days, download_path=\"glofas_2023\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["download_era5_historic(copernicus,bounds=europe,years=years,months=months,days=days, download_path=\"era5_2023\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#era5_zip = save_binary_to_cos('era5_2023.netcdf.zip', 'era5_2023.netcdf.zip')\n","#glofas_zip = save_binary_to_cos('glofas_2023.netcdf.zip', 'glofas_2023.netcdf.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["!mkdir era5 && mkdir glofas"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!unzip era5_2023.netcdf.zip -d era5 && unzip glofas_2023.netcdf4.zip -d glofas"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["e5 = xr.open_dataset('era5/data.nc')\n","f = xr.open_dataset('glofas/data.nc')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Handle ERA5 Data\n","\n","**Data**: Total Precipitation; Volumetric Soil Water Layer 1; Soil Temperature Level 1\n","\n","**Mission**: We requested the above mentioned variables for roughly the same coordinates (variation of .05). Lets have a quick look at the dataset and prepare it for a training split, version control, and more.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["e5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Interpolate to drop 'expver' mask from coordinates\n","e5_interp = e5.interp_like(f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["e5_interp"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get rid of that darn supplementary expver dimension's issue (See https://confluence.ecmwf.int/display/CUSF/ERA5+CDS+requests+which+return+a+mixture+of+ERA5+and+ERA5T+data)\n","e5_combine = e5_interp.sel(expver=1).combine_first(e5_interp.sel(expver=5))\n","e5_combine.load()\n","e5_combine"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = e5_combine[['tp', 'stl1', 'swvl1']]\n","y = f['dis24']\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = e5_combine[['stl1', 'tp', 'swvl1']].to_dataframe()\n","y = f['dis24'].to_dataframe()\n","\n","# Reset the index to include the coordinates as columns\n","X.reset_index(inplace=True)\n","y.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Merge X and y on the common coordinates (time, latitude, longitude)\n","data = pd.merge(X, y, on=['time', 'latitude', 'longitude'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data['time'].max()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Pickle and save data\n","\n","FILENAME = \"era5-glofas-merged.pkl\"\n","\n","save_df_to_cos(data, FILENAME, FILENAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["files_copied_in_cos = check_if_file_exists(FILENAME)\n","files_copied_in_cos"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Hand-off to Next Pipeline Node"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validation_params = {}\n","validation_params['most_recent_day_in_data'] = str(data['time'].max()).split()[0] # Shows most recent day covered by data ('2023-04-30')\n","validation_params['serialized_data_filename'] = \"era5-glofas-merged.pkl\"\n","validation_params['files_copied_in_cos'] = files_copied_in_cos"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipelines_client = WSPipelines.from_apikey(apikey=CLOUD_API_KEY)\n","pipelines_client.store_results(validation_params)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Make train test split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Perform train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(data['stl1'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_wo_precip = data.dropna(subset=['tp'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_wo_precip"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_wo_precip.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_wo_2 = data_wo_precip.dropna(subset=['swvl1'])\n","data_wo_2.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":1}
