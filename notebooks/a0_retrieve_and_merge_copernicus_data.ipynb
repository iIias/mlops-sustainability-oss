{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Retrieve and Concatenate Copernicus Data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### üéØ Request Data from Copernicus (GloFAS; ERA5) and Save Concatenated Data to Cloud Object Storage (COS)\n","\n"," We use the [Copernicus Data Store](https://cds.climate.copernicus.eu/#!/home) to retrieve historic and current climate data. We are collecting the following variables from the ERA5 and GloFAS datasets:\n","\n","- üåç [ERA5-Land hourly data from 1950 to present](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land?tab=form)\n","    - ```stl1``` (Soil Temperature Level 1)\n","    - ```vswl1``` (Volumetric Soil Water Layer 1)\n","    - ```total_preciptation``` (Total Precipitation)\n","\n","- üåä [River discharge and related historical data from the Global Floow Awareness System (GloFAS)](https://cds.climate.copernicus.eu/cdsapp#!/dataset/cems-glofas-historical?tab=form)\n","    - ```dis24``` (averaged daily river discharge in m^3/s)\n","\n","We want to concatenate the variables of the two tables of the spatio-temporal common (interpolated) columns (e.g. ```latitude```, ```longitude```, ```time```)\n","\n","**In this notebook, we assume that when it is run, there is no notion of our data since it has never been persisted. Therefore this is the *initial* notebook to run and ideally only run once.** When invoking the pipeline further times there should already be historic data in place, which makes running this notebook unnecessary (at that point).\n","\n","#### Steps covered in this notebook:\n","1. Retrieve parameters & Set-up Cloud Object Storage connection\n","2. Set-up Copernicus credentials (w/ Configuration File)\n","3. **Retrieve ERA5 and GloFAS for given timeframe**\n","4. Handle both netcdf files (open, interpolate, reset_index, to_pandas)\n","5. **Concatenate both datasets on Latitude, Longitude, Time**\n","6. Serialize result and persist with Cloud Object Storage"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: Create software configuration in Watson Studio to reduce resource waste by installing manually on each run\n","!pip install cdsapi netCDF4 xarray ibm_watson_studio_pipelines scikit-learn==1.1 ibm-cos-sdk botocore"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# data sources\n","import cdsapi\n","\n","# data manipulation\n","from netCDF4 import Dataset\n","import xarray as xr\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# serialization\n","import pickle\n","import json\n","\n","# remotes\n","from botocore.client import Config\n","from ibm_watson_studio_pipelines import WSPipelines\n","import ibm_boto3\n","\n","# misc\n","import logging\n","import os, types\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","\n","\n","### 1. Retrieve parameters & Set-up Cloud Object Storage connection\n","\n","**Note**: If you are running this notebook outside of a Watson Studio Pipeline execution. Make sure to set the environment variables that the Pipeline environment would have passed to the notebook.\n","Refer to ```credentials.py```."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Uncomment this cell and put your credentials in credentials.py to run locally.\n","from credentials import set_env_variables_for_credentials\n","set_env_variables_for_credentials()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Retrieve cos credentials from global pipeline parameters\n","\n","# Get json from environment and convert to string\n","project_cos_credentials = json.loads(os.getenv('PROJECT_COS_CREDENTIALS'))\n","mlops_cos_credentials = json.loads(os.getenv('MLOPS_COS_CREDENTIALS'))\n","\n","## PROJECT COS \n","AUTH_ENDPOINT = project_cos_credentials['AUTH_ENDPOINT']\n","ENDPOINT_URL = project_cos_credentials['ENDPOINT_URL']\n","API_KEY_COS = project_cos_credentials['API_KEY']\n","BUCKET_PROJECT_COS = project_cos_credentials['BUCKET']\n","\n","## MLOPS COS\n","ENDPOINT_URL_MLOPS = mlops_cos_credentials['ENDPOINT_URL']\n","API_KEY_MLOPS = mlops_cos_credentials['API_KEY']\n","CRN_MLOPS = mlops_cos_credentials['CRN']\n","BUCKET_MLOPS  = mlops_cos_credentials['BUCKET']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CLOUD_API_KEY = os.getenv('CLOUD_API_KEY')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_df_to_cos(df,filename,key):\n","    \"\"\"\n","    \n","    Save Data in IBM Cloud Object Storage\n","\n","    \n","    \"\"\"\n","\n","    try:\n","        #df.to_csv(filename,index=False)\n","        with open(filename, 'wb') as file:\n","            pickle.dump(df, file)\n","        mlops_res = ibm_boto3.resource(\n","            service_name='s3',\n","            ibm_api_key_id=API_KEY_MLOPS,\n","            ibm_service_instance_id=CRN_MLOPS,\n","            ibm_auth_endpoint=AUTH_ENDPOINT,\n","            config=Config(signature_version='oauth'),\n","            endpoint_url=ENDPOINT_URL_MLOPS)\n","\n","        mlops_res.Bucket(BUCKET_MLOPS).upload_file(filename,key)\n","        print(f\"Dataframe {filename} uploaded successfully\")\n","    except Exception as e:\n","        print(e)\n","        print(\"Dataframe upload for {filename} failed\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Set-up Copernicus credentials (w/ Configuration File)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The Python Library for the Copernicus API (```cdsapi```) handles service authentication via a configuration file in the users home directory. <br>Hardcode the ```CDS_USER_ID``` and ```CDS_API_KEY``` environment variables in your ```credentials.py```, or preferably pass them as Pipeline Parameters within Watson Studio.\n","\n","The code below will take the passed env. variables and write the configuration file to your home dir."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use your Copernicus API_KEY\n","# @hidden_cell\n","import os\n","CDS_USER_ID = os.getenv(\"CDS_USER_ID\")\n","CDS_API_KEY = os.getenv(\"CDS_API_KEY\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setup copernicus credentials file for cdsapi\n","import os\n","with open(os.path.join(os.path.expanduser('~'), '.cdsapirc'), 'w') as f:\n","    f.write('url: https://cds.climate.copernicus.eu/api/v2\\n')\n","    f.write(f'key: {CDS_USER_ID}:{CDS_API_KEY}')"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["# Ensure COPERNICUS config is setup at the right place\n","!cat ~/.cdsapirc"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The ```cdsapi.Client``` initialized below, will check for the existence of the  configuration file created above, and for the correctness of the credentials it houses. If neither of these applies, the below cell will run into an exception."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["copernicus = cdsapi.Client()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Retrieve ERA5 and GloFAS for given timeframe"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The amount of data requested for either dataset may be delimited by the selection of various data variables, as well as by setting spatial and/or temporal  bounds for the download request."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["europe = [72,25,34,40] # NWSE bounds for Europe\n","days = [str(i) for i in range(31)]\n","# months = ['january', 'february', 'march', 'april']\n","# years = ['2023']\n","\n","months = ['january', 'february', 'march', 'april']#, 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n","years = ['2023']\n","\n","hours = [\n","            '00:00', '01:00', '02:00',\n","            '03:00', '04:00', '05:00',\n","            '06:00', '07:00', '08:00',\n","            '09:00', '10:00', '11:00',\n","            '12:00', '13:00', '14:00',\n","            '15:00', '16:00', '17:00',\n","            '18:00', '19:00', '20:00',\n","            '21:00', '22:00', '23:00',\n","]\n","\n","hours = [ '00:00',]\n","hours"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def download_glofas_historic(client, bounds, years, months, days, download_path):\n","    glofas_format = \".netcdf4.zip\"\n","    if os.path.exists(f'{download_path}{glofas_format}'):\n","        # Reason to cancel download process if file exists is elaborated where method is invoked.\n","        print(f\"Target filename already exists in target path ({download_path}{glofas_format})... cancelling download\")\n","        exit\n","    else:\n","        client.retrieve(\n","            'cems-glofas-historical',\n","            {\n","                'system_version': 'version_3_1',\n","                'variable': 'river_discharge_in_the_last_24_hours',\n","                'format': 'netcdf4.zip',\n","                'hyear': years,\n","                'hmonth': months,\n","                'hday': days,\n","                'hydrological_model': 'lisflood',\n","                'product_type': 'intermediate',\n","                'area': bounds,\n","            },\n","            f'{download_path}.netcdf4.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download ERA5 monthly averaged data from soil temp l1, volumetric soil water l1, total precipitation\n","def download_era5_historic(client, bounds, years, months, days, hours, download_path):\n","    era5_format = \".netcdf.zip\"\n","    if os.path.exists(f'{download_path}{era5_format}'):\n","        # Reason to cancel download process if file exists is elaborated where method is invoked.\n","        print(f\"Target filename already exists in target path ({download_path}{era5_format})... cancelling download\")\n","        exit\n","    else:\n","        client.retrieve(\n","            'reanalysis-era5-land',\n","            {\n","                'variable': [\n","                    'soil_temperature_level_1', 'total_precipitation', 'volumetric_soil_water_layer_1',\n","                ],\n","                'year': years,\n","                # CDS Datasets do not have uniformal requests. Here Months are expected to be e.g. \"01\" instead of 'january'.\n","                # Work-around with list comprehension\n","                # 'month': [str(i) for i in range(len(months))],\n","                'month': [f'0{i+1}' if i < 9 else str(i+1) for i in range(len(months))],\n","                'day': [f'0{i+1}' if i < 9 else str(i+1) for i in range(len(days))],\n","                'time': hours,\n","                'format': 'netcdf.zip',\n","                'area': bounds,\n","            },\n","            f'{download_path}.netcdf.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# NOTE: cdsapi has no notion of the files in the current working directory. \n","# Passing a download path and filename where a file already sits causes a seemingly infinite loop in the download process.\n","# Your cell will never finish running and resources will be wasted.\n","# No problem for CPDaaS since working directory is runtime bound (no persistent filesystem) and in production the file cannot already exist.\n","download_glofas_historic(\n","    copernicus,\n","    bounds=europe,\n","    years=years,\n","    months=months,\n","    days=days,\n","    download_path=\"glofas_2023\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# NOTE: cdsapi has no notion of the files in the current working directory. \n","# Passing a download path and filename where a file already sits causes a seemingly infinite loop in the download process.\n","# Your cell will never finish running and resources will be wasted.\n","# No problem for CPDaaS since working directory is runtime bound (no persistent filesystem) and in production the file cannot already exist.\n","download_era5_historic(\n","    copernicus,\n","    bounds=europe,\n","    years=years,\n","    months=months,\n","    days=days,\n","    hours=hours,\n","    download_path=\"era5_2023\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!ls -lh"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Handle ERA5/GloFAS netcdf files (open, interpolate, reset_index, to_pandas)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["!mkdir era5 && mkdir glofas"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!unzip era5_2023.netcdf.zip -d era5 && unzip glofas_2023.netcdf4.zip -d glofas"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["e5 = xr.open_dataset('era5/data.nc')\n","f = xr.open_dataset('glofas/data.nc')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Handle ERA5 Data\n","\n","**Data**: Total Precipitation; Volumetric Soil Water Layer 1; Soil Temperature Level 1\n","\n","**Mission**: We requested the above mentioned variables for roughly the same coordinates (variation of .05). Lets have a quick look at the dataset and prepare it for a training split, version control, and more.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["e5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Interpolate to drop 'expver' mask from coordinates\n","e5_interp = e5.interp_like(f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["e5_interp"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get rid of that darn supplementary expver dimension's issue (See https://confluence.ecmwf.int/display/CUSF/ERA5+CDS+requests+which+return+a+mixture+of+ERA5+and+ERA5T+data)\n","e5_combine = e5_interp.sel(expver=1).combine_first(e5_interp.sel(expver=5))\n","e5_combine.load()\n","e5_combine"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Concatenate both datasets on common columns: Latitude, Longitude, Time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Joining predictand onto feature y-interpolated table \n","# Set features to keep and choose target variable\n","X = e5_combine.to_dataframe()\n","y = f['dis24'].to_dataframe()\n","\n","# Reset the index to include the coordinates as columns\n","X.reset_index(inplace=True)\n","y.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Merge features and predictand together common coordinates (time, latitude, longitude)\n","data = pd.merge(X, y, on=['time', 'latitude', 'longitude'])\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Shows most recent day covered by data ('2023-04-30') to later handle merging with newer data more efficiently\n","most_recent_covered_day = str(data['time'].max()).split()[0] "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Serialize Concatenated Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Pickle and save data\n","\n","FILENAME = \"era5-glofas-merged.pkl\"\n","\n","save_df_to_cos(data, FILENAME, FILENAME)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Persist on Cloud Object Storage\n","\n","Serialized dataset will be moved to COS since filesystem in CPDaaS runtimes is temporary and therefore unfit to house our data. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["files_copied_in_cos = check_if_file_exists(FILENAME)\n","files_copied_in_cos"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Hand-off to Next Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validation_params = {}\n","validation_params['most_recent_day_in_data'] = most_recent_covered_day\n","validation_params['serialized_data_filename'] = FILENAME\n","validation_params['files_copied_in_cos'] = files_copied_in_cos"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipelines_client = WSPipelines.from_apikey(apikey=CLOUD_API_KEY)\n","pipelines_client.store_results(validation_params)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":1}
